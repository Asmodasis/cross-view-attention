{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:16:54.569748Z",
     "iopub.status.busy": "2022-12-02T04:16:54.569385Z",
     "iopub.status.idle": "2022-12-02T04:17:03.975780Z",
     "shell.execute_reply": "2022-12-02T04:17:03.974408Z",
     "shell.execute_reply.started": "2022-12-02T04:16:54.569717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_applications in d:\\users\\asmod\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: h5py in d:\\users\\asmod\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from keras_applications) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in d:\\users\\asmod\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from keras_applications) (1.23.3)\n",
      "All modules have been imported\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import cv2\n",
    "#import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.utils import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from skimage.io import *\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Softmax\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "!pip install keras_applications\n",
    "\n",
    "import keras_applications\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "print(\"All modules have been imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:03.978937Z",
     "iopub.status.busy": "2022-12-02T04:17:03.978512Z",
     "iopub.status.idle": "2022-12-02T04:17:03.984358Z",
     "shell.execute_reply": "2022-12-02T04:17:03.983376Z",
     "shell.execute_reply.started": "2022-12-02T04:17:03.978890Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_percent = 100 # percent of original size\n",
    "width = int(np.round(600 * scale_percent / 100))\n",
    "height = int(np.round(300 * scale_percent / 100))\n",
    "dim = (width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################<br>\n",
    "# ResNet50 Model Definition ###<br>\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:03.986716Z",
     "iopub.status.busy": "2022-12-02T04:17:03.986008Z",
     "iopub.status.idle": "2022-12-02T04:17:04.007481Z",
     "shell.execute_reply": "2022-12-02T04:17:04.006401Z",
     "shell.execute_reply.started": "2022-12-02T04:17:03.986680Z"
    }
   },
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:04.011100Z",
     "iopub.status.busy": "2022-12-02T04:17:04.010289Z",
     "iopub.status.idle": "2022-12-02T04:17:04.020822Z",
     "shell.execute_reply": "2022-12-02T04:17:04.019872Z",
     "shell.execute_reply.started": "2022-12-02T04:17:04.011066Z"
    }
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:04.022762Z",
     "iopub.status.busy": "2022-12-02T04:17:04.022334Z",
     "iopub.status.idle": "2022-12-02T04:17:04.034483Z",
     "shell.execute_reply": "2022-12-02T04:17:04.033491Z",
     "shell.execute_reply.started": "2022-12-02T04:17:04.022728Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n",
    "    And the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:04.036695Z",
     "iopub.status.busy": "2022-12-02T04:17:04.036062Z",
     "iopub.status.idle": "2022-12-02T04:17:04.061591Z",
     "shell.execute_reply": "2022-12-02T04:17:04.060608Z",
     "shell.execute_reply.started": "2022-12-02T04:17:04.036654Z"
    }
   },
   "outputs": [],
   "source": [
    "def cvam_block(x,x1,x2, ratio=8):\n",
    "  \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "  As described in https://arxiv.org/abs/1807.06521.\n",
    "  \"\"\"\n",
    "  #print('before',x)\n",
    "  c_map = channel_attention2(x,x2, ratio)\n",
    "  #print ('after',x_enhanced)\n",
    "  s_map = spatial_attention2(x,x1)\n",
    "  return multiply([x, 1+s_map,1+c_map])\n",
    "  #x=RMLO\n",
    "  #x1=LMLO\n",
    "  #x2=RCC\n",
    "  #x3=LCC\n",
    "def channel_attention2(x,x2, ratio=8):\n",
    "  features=[x,x2]\n",
    "  channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "  channel = x.shape[channel_axis]\t\n",
    "  #channel = input_feature.shape\n",
    "  shared_layer_one = Dense(channel//ratio,\n",
    "              activation='relu',\n",
    "              kernel_initializer='he_normal',\n",
    "              use_bias=True,\n",
    "              bias_initializer='zeros')\n",
    "  shared_layer_two = Dense(channel,\n",
    "              kernel_initializer='he_normal',\n",
    "              use_bias=True,\n",
    "              bias_initializer='zeros')\n",
    "\n",
    "  avg_pool = GlobalAveragePooling2D()(x)    \n",
    "  avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "  assert avg_pool.shape[1:] == (1,1,channel)\n",
    "  avg_pool = shared_layer_one(avg_pool)\n",
    "  assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "  avg_pool = shared_layer_two(avg_pool)\n",
    "  assert avg_pool.shape[1:] == (1,1,channel)\n",
    "\n",
    "  max_pool = GlobalMaxPooling2D()(x)\n",
    "  max_pool = Reshape((1,1,channel))(max_pool)\n",
    "  assert max_pool.shape[1:] == (1,1,channel)\n",
    "  max_pool = shared_layer_one(max_pool)\n",
    "  assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "  max_pool = shared_layer_two(max_pool)\n",
    "  assert max_pool.shape[1:] == (1,1,channel)\n",
    "\n",
    "  avg_pool1 = GlobalAveragePooling2D()(x2)    \n",
    "  avg_pool1 = Reshape((1,1,channel))(avg_pool1)\n",
    "  assert avg_pool1.shape[1:] == (1,1,channel)\n",
    "  avg_pool1 = shared_layer_one(avg_pool1)\n",
    "  assert avg_pool1.shape[1:] == (1,1,channel//ratio)\n",
    "  avg_pool1 = shared_layer_two(avg_pool1)\n",
    "  assert avg_pool1.shape[1:] == (1,1,channel)\n",
    "\n",
    "  max_pool1 = GlobalMaxPooling2D()(x2)\n",
    "  max_pool1 = Reshape((1,1,channel))(max_pool1)\n",
    "  assert max_pool1.shape[1:] == (1,1,channel)\n",
    "  max_pool1 = shared_layer_one(max_pool1)\n",
    "  assert max_pool1.shape[1:] == (1,1,channel//ratio)\n",
    "  max_pool1 = shared_layer_two(max_pool1)\n",
    "  assert max_pool1.shape[1:] == (1,1,channel)\n",
    "\n",
    "  concat = Concatenate(axis=3)([avg_pool, max_pool, avg_pool1, max_pool1])\n",
    "  print('concat', concat)\n",
    "  print('avg_pool',avg_pool)\n",
    "  cbam_feature=Dense(x.shape[3])(concat)\n",
    "  print ('after dense1',cbam_feature)\n",
    "  cbam_feature = Activation('relu')(cbam_feature)\n",
    "  cbam_feature=Dense(x.shape[3])(concat)\n",
    "  print ('after dense2',cbam_feature)\n",
    "  #sum = Add()([avg_pool,max_pool,avg_pool1,max_pool1])\n",
    "  #print ('sum', sum)\n",
    "  cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "  print (cbam_feature)\n",
    "  if K.image_data_format() == \"channels_first\":\n",
    "    cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "  \n",
    "  return cbam_feature\n",
    "\n",
    "def spatial_attention2(x,x1):\n",
    "  kernel_size = 7\n",
    "\t\n",
    "  if K.image_data_format() == \"channels_first\":\n",
    "    channel = x.shape[1]\n",
    "    cbam_feature = Permute((2,3,1))(x)\n",
    "  else:\n",
    "    channel = x.shape[-1]\n",
    "    cbam_feature = x\n",
    "\t\n",
    "  avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "  print ('avg pool v1',avg_pool)\n",
    "  assert avg_pool.shape[-1] == 1\n",
    "  max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "  assert max_pool.shape[-1] == 1\n",
    "\n",
    "  if K.image_data_format() == \"channels_first\":\n",
    "    channel = x1.shape[1]\n",
    "    cbam_feature = Permute((2,3,1))(x1)\n",
    "  else:\n",
    "    channel = x1.shape[-1]\n",
    "    cbam_feature = x1\n",
    "\n",
    "  avg_pool1 = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(x1)\n",
    "  print ('avg pool v2',avg_pool1)\n",
    "  assert avg_pool1.shape[-1] == 1\n",
    "  max_pool1 = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(x1)\n",
    "  assert max_pool1.shape[-1] == 1\n",
    "\n",
    "  concat = Concatenate(axis=3)([avg_pool, max_pool, avg_pool1, max_pool1])\n",
    "  #concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "  assert concat.shape[-1] == 4\n",
    "  cbam_feature = Conv2D(4, (3, 3), padding=\"same\")(concat)\n",
    "  cbam_feature = Activation('relu')(cbam_feature)\t\n",
    "  cbam_feature = Conv2D(1, (3, 3), padding=\"same\")(cbam_feature)\n",
    "  cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "  #print (cbam_feature)\t\n",
    "  assert cbam_feature.shape[-1] == 1\n",
    "  if K.image_data_format() == \"channels_first\":\n",
    "    cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "  #print (multiply([x, cbam_feature]))\n",
    "  return cbam_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:04.064535Z",
     "iopub.status.busy": "2022-12-02T04:17:04.063886Z",
     "iopub.status.idle": "2022-12-02T04:17:04.116623Z",
     "shell.execute_reply": "2022-12-02T04:17:04.115598Z",
     "shell.execute_reply.started": "2022-12-02T04:17:04.064501Z"
    }
   },
   "outputs": [],
   "source": [
    "def MyResNet50(dimension,include_top=True, weights='imagenet',\n",
    "             input_tensor=None, input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1):\n",
    "    \"\"\"Instantiates the ResNet50 architecture.\n",
    "    Optionally loads weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format=\"channels_last\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "    # Arguments\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 244)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 197.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    #######################################\n",
    "    ##### Determine proper input shape ####\n",
    "    #######################################\n",
    "    #input_shape = [dimension[0],dimension[1],4,3]\n",
    "    #input_shape = [dimension[1],dimension[0],3,4]\n",
    "    input_shape = [4,dimension[0],dimension[1],3]\n",
    "    #print (\"input_shape : \", input_shape)\n",
    "    #print (input_tensor)\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    #print ('before',img_input)\n",
    "    #input_tensor=input_tensor[:,:,:,:,0]\n",
    "#     x  =img_input[0]\n",
    "#     x1 =img_input[1]\n",
    "#     x2 =img_input[2]\n",
    "#     print (x2)\n",
    "    x=img_input[:,0,:,:,:]\n",
    "    x1=img_input[:,1,:,:,:]\n",
    "    x2=img_input[:,2,:,:,:]\n",
    "    \n",
    "    x3=img_input[:,3,:,:,:]\n",
    "\n",
    "    #print ('after',x)\n",
    "    x = ZeroPadding2D((3, 3))(x)\n",
    "    x1 = ZeroPadding2D((3, 3))(x1)\n",
    "    x2 = ZeroPadding2D((3, 3))(x2)\n",
    "    x3 = ZeroPadding2D((3, 3))(x3)\n",
    "    #print ('first', x)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n",
    "    x1 = Conv2D(64, (7, 7), strides=(2, 2), name='conv1_1')(x1)\n",
    "    x2 = Conv2D(64, (7, 7), strides=(2, 2), name='conv1_2')(x2)\n",
    "    x3 = Conv2D(64, (7, 7), strides=(2, 2), name='conv1_3')(x3)\n",
    "\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x1 = BatchNormalization(axis=bn_axis, name='bn_conv1_1')(x1)\n",
    "    x2 = BatchNormalization(axis=bn_axis, name='bn_conv1_2')(x2)\n",
    "    x3 = BatchNormalization(axis=bn_axis, name='bn_conv1_3')(x3)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x2 = Activation('relu')(x2)\n",
    "    x3 = Activation('relu')(x3)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x1 = MaxPooling2D((3, 3), strides=(2, 2))(x1)\n",
    "    x2 = MaxPooling2D((3, 3), strides=(2, 2))(x2)\n",
    "    x3 = MaxPooling2D((3, 3), strides=(2, 2))(x3)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x1 = conv_block(x1, 3, [64, 64, 256], stage=2, block='a1', strides=(1, 1))\n",
    "    x2 = conv_block(x2, 3, [64, 64, 256], stage=2, block='a2', strides=(1, 1))\n",
    "    x3 = conv_block(x3, 3, [64, 64, 256], stage=2, block='a3', strides=(1, 1))\n",
    "\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x1 = identity_block(x1, 3, [64, 64, 256], stage=2, block='b1')\n",
    "    x2 = identity_block(x2, 3, [64, 64, 256], stage=2, block='b2')\n",
    "    x3 = identity_block(x3, 3, [64, 64, 256], stage=2, block='b3')\n",
    "\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "    x1 = identity_block(x1, 3, [64, 64, 256], stage=2, block='c1')\n",
    "    x2 = identity_block(x2, 3, [64, 64, 256], stage=2, block='c2')\n",
    "    x3 = identity_block(x3, 3, [64, 64, 256], stage=2, block='c3')\n",
    "    #print ('x1',x1.shape[3])\n",
    "    \n",
    "    #x = cbam_block(x)\n",
    "\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x1 = conv_block(x1, 3, [128, 128, 512], stage=3, block='a1')\n",
    "    x2 = conv_block(x2, 3, [128, 128, 512], stage=3, block='a2')\n",
    "    x3 = conv_block(x2, 3, [128, 128, 512], stage=3, block='a3')\n",
    "\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x1 = identity_block(x1, 3, [128, 128, 512], stage=3, block='b1')\n",
    "    x2 = identity_block(x2, 3, [128, 128, 512], stage=3, block='b2')\n",
    "    x3 = identity_block(x3, 3, [128, 128, 512], stage=3, block='b3')\n",
    "\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x1 = identity_block(x1, 3, [128, 128, 512], stage=3, block='c1')\n",
    "    x2 = identity_block(x2, 3, [128, 128, 512], stage=3, block='c2')\n",
    "    x3 = identity_block(x3, 3, [128, 128, 512], stage=3, block='c3')\n",
    "\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "    x1 = identity_block(x1, 3, [128, 128, 512], stage=3, block='d1')\n",
    "    x2 = identity_block(x2, 3, [128, 128, 512], stage=3, block='d2')\n",
    "    x3 = identity_block(x3, 3, [128, 128, 512], stage=3, block='d3')\n",
    "\n",
    "    #x = cbam_block(x)\n",
    "\n",
    "    x = cvam_block(x,x1,x2)\n",
    "    #x1 = cvam_block(x1,x,x3)\n",
    "    #x2 = cvam_block(x2,x3,x)\n",
    "    # x3 = cvam_block(x3,x2,x1)\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x1 = conv_block(x1, 3, [256, 256, 1024], stage=4, block='a1')\n",
    "    x2 = conv_block(x2, 3, [256, 256, 1024], stage=4, block='a2')\n",
    "    x3 = conv_block(x3, 3, [256, 256, 1024], stage=4, block='a3')\n",
    "\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x1 = identity_block(x1, 3, [256, 256, 1024], stage=4, block='b1')\n",
    "    x2 = identity_block(x2, 3, [256, 256, 1024], stage=4, block='b2')\n",
    "    x3 = identity_block(x3, 3, [256, 256, 1024], stage=4, block='b3')\n",
    "\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x1 = identity_block(x1, 3, [256, 256, 1024], stage=4, block='c1')\n",
    "    x2 = identity_block(x2, 3, [256, 256, 1024], stage=4, block='c2')\n",
    "    x3 = identity_block(x3, 3, [256, 256, 1024], stage=4, block='c3')\n",
    "\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x1 = identity_block(x1, 3, [256, 256, 1024], stage=4, block='d1')\n",
    "    x2 = identity_block(x2, 3, [256, 256, 1024], stage=4, block='d2')\n",
    "    x3 = identity_block(x3, 3, [256, 256, 1024], stage=4, block='d3')\n",
    "\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x1 = identity_block(x1, 3, [256, 256, 1024], stage=4, block='e1')\n",
    "    x2 = identity_block(x2, 3, [256, 256, 1024], stage=4, block='e2')\n",
    "    x3 = identity_block(x3, 3, [256, 256, 1024], stage=4, block='e3')\n",
    "\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "    x1 = identity_block(x1, 3, [256, 256, 1024], stage=4, block='f1')\n",
    "    x2 = identity_block(x2, 3, [256, 256, 1024], stage=4, block='f2')\n",
    "    x3 = identity_block(x3, 3, [256, 256, 1024], stage=4, block='f3')\n",
    "\n",
    "    #x = cbam_block(x)\n",
    "    x = cvam_block(x,x1,x2)\n",
    "    #x1 = cvam_block(x1,x,x3)\n",
    "    #x2 = cvam_block(x2,x3,x)\n",
    "    # x3 = cvam_block(x3,x2,x1)\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x1 = conv_block(x1, 3, [512, 512, 2048], stage=5, block='a1')\n",
    "    x2 = conv_block(x2, 3, [512, 512, 2048], stage=5, block='a2')\n",
    "    x3 = conv_block(x3, 3, [512, 512, 2048], stage=5, block='a3')\n",
    "\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x1 = identity_block(x1, 3, [512, 512, 2048], stage=5, block='b1')\n",
    "    x2 = identity_block(x2, 3, [512, 512, 2048], stage=5, block='b2')\n",
    "    x3 = identity_block(x3, 3, [512, 512, 2048], stage=5, block='b3')\n",
    "\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    x1 = identity_block(x1, 3, [512, 512, 2048], stage=5, block='c1')\n",
    "    x2 = identity_block(x2, 3, [512, 512, 2048], stage=5, block='c2')\n",
    "    x3 = identity_block(x3, 3, [512, 512, 2048], stage=5, block='c3')\n",
    "\n",
    "    #x = cbam_block(x)\n",
    "    x = cvam_block(x,x1,x2)\n",
    "    #x1 = cvam_block(x1,x,x3)\n",
    "    #x2 = cvam_block(x2,x3,x)\n",
    "    # x3 = cvam_block(x3,x2,x1)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x1 = GlobalAveragePooling2D()(x1)\n",
    "    x2 = GlobalAveragePooling2D()(x2)\n",
    "    x3 = GlobalAveragePooling2D()(x3)\n",
    "    \n",
    "    x = Dense(256,activation='PReLU')(x)\n",
    "    x1 = Dense(256,activation='PReLU')(x1)\n",
    "    x2 = Dense(256,activation='PReLU')(x2)\n",
    "    x3 = Dense(256,activation='PReLU')(x3)\n",
    "    \n",
    "    x = Dropout(.5)(x)\n",
    "    x1 = Dropout(.5)(x1)\n",
    "    x2 = Dropout(.5)(x2)\n",
    "    x3 = Dropout(.5)(x3)\n",
    "    \n",
    "    x=BatchNormalization()(x)\n",
    "    x1=BatchNormalization()(x1)\n",
    "    x2=BatchNormalization()(x2)\n",
    "    x3=BatchNormalization()(x3)\n",
    "    \n",
    "    x = Dense(256,activation='PReLU')(x)\n",
    "    x1 = Dense(256,activation='PReLU')(x1)\n",
    "    x2 = Dense(256,activation='PReLU')(x2)\n",
    "    x3 = Dense(256,activation='PReLU')(x3)\n",
    "    \n",
    "    x = Dropout(.5)(x)\n",
    "    x1 = Dropout(.5)(x1)\n",
    "    x2 = Dropout(.5)(x2)\n",
    "    x3 = Dropout(.5)(x3)\n",
    "    \n",
    "    x=BatchNormalization()(x)\n",
    "    x1=BatchNormalization()(x1)\n",
    "    x2=BatchNormalization()(x2)\n",
    "    x3=BatchNormalization()(x3)\n",
    "    \n",
    "    x = Dense(3,activation='sigmoid')(x)\n",
    "    x1 = Dense(3,activation='sigmoid')(x1)\n",
    "    x2 = Dense(3,activation='sigmoid')(x2)\n",
    "    x3 = Dense(3,activation='sigmoid')(x3)\n",
    "    \n",
    "    x=tf.concat([x,x1,x2,x3],1)\n",
    "    print(x)\n",
    "\n",
    "    #print (input_tensor, img_input)\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs=inputs, outputs=x, name='resnet50')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models',\n",
    "                                    md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n",
    "        else:\n",
    "            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models',\n",
    "                                    md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "        model.load_weights(weights_path, by_name = True, skip_mismatch = True)\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='avg_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1000')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you ')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:04.118131Z",
     "iopub.status.busy": "2022-12-02T04:17:04.117776Z",
     "iopub.status.idle": "2022-12-02T04:17:09.209171Z",
     "shell.execute_reply": "2022-12-02T04:17:09.208105Z",
     "shell.execute_reply.started": "2022-12-02T04:17:04.118097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 2048), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n",
      "avg_pool KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 512), dtype=tf.float32, name=None), name='dense_1/BiasAdd:0', description=\"created by layer 'dense_1'\")\n",
      "after dense1 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 512), dtype=tf.float32, name=None), name='dense_2/BiasAdd:0', description=\"created by layer 'dense_2'\")\n",
      "after dense2 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 512), dtype=tf.float32, name=None), name='dense_3/BiasAdd:0', description=\"created by layer 'dense_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 512), dtype=tf.float32, name=None), name='activation_89/Sigmoid:0', description=\"created by layer 'activation_89'\")\n",
      "avg pool v1 KerasTensor(type_spec=TensorSpec(shape=(None, 75, 37, 1), dtype=tf.float32, name=None), name='lambda/Mean:0', description=\"created by layer 'lambda'\")\n",
      "avg pool v2 KerasTensor(type_spec=TensorSpec(shape=(None, 75, 37, 1), dtype=tf.float32, name=None), name='lambda_2/Mean:0', description=\"created by layer 'lambda_2'\")\n",
      "concat KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 4096), dtype=tf.float32, name=None), name='concatenate_2/concat:0', description=\"created by layer 'concatenate_2'\")\n",
      "avg_pool KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 1024), dtype=tf.float32, name=None), name='dense_5/BiasAdd:0', description=\"created by layer 'dense_5'\")\n",
      "after dense1 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 1024), dtype=tf.float32, name=None), name='dense_6/BiasAdd:0', description=\"created by layer 'dense_6'\")\n",
      "after dense2 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 1024), dtype=tf.float32, name=None), name='dense_7/BiasAdd:0', description=\"created by layer 'dense_7'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 1024), dtype=tf.float32, name=None), name='activation_165/Sigmoid:0', description=\"created by layer 'activation_165'\")\n",
      "avg pool v1 KerasTensor(type_spec=TensorSpec(shape=(None, 38, 19, 1), dtype=tf.float32, name=None), name='lambda_4/Mean:0', description=\"created by layer 'lambda_4'\")\n",
      "avg pool v2 KerasTensor(type_spec=TensorSpec(shape=(None, 38, 19, 1), dtype=tf.float32, name=None), name='lambda_6/Mean:0', description=\"created by layer 'lambda_6'\")\n",
      "concat KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 8192), dtype=tf.float32, name=None), name='concatenate_4/concat:0', description=\"created by layer 'concatenate_4'\")\n",
      "avg_pool KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 2048), dtype=tf.float32, name=None), name='dense_9/BiasAdd:0', description=\"created by layer 'dense_9'\")\n",
      "after dense1 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 2048), dtype=tf.float32, name=None), name='dense_10/BiasAdd:0', description=\"created by layer 'dense_10'\")\n",
      "after dense2 KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 2048), dtype=tf.float32, name=None), name='dense_11/BiasAdd:0', description=\"created by layer 'dense_11'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 2048), dtype=tf.float32, name=None), name='activation_205/Sigmoid:0', description=\"created by layer 'activation_205'\")\n",
      "avg pool v1 KerasTensor(type_spec=TensorSpec(shape=(None, 19, 10, 1), dtype=tf.float32, name=None), name='lambda_8/Mean:0', description=\"created by layer 'lambda_8'\")\n",
      "avg pool v2 KerasTensor(type_spec=TensorSpec(shape=(None, 19, 10, 1), dtype=tf.float32, name=None), name='lambda_10/Mean:0', description=\"created by layer 'lambda_10'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='tf.concat/concat:0', description=\"created by layer 'tf.concat'\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = MyResNet50(dim,weights='imagenet', include_top=False)\n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################<br>\n",
    "## Define Preprocessing Functions ####<br>\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:12.855713Z",
     "iopub.status.busy": "2022-12-02T04:17:12.855301Z",
     "iopub.status.idle": "2022-12-02T04:17:12.862956Z",
     "shell.execute_reply": "2022-12-02T04:17:12.860888Z",
     "shell.execute_reply.started": "2022-12-02T04:17:12.855679Z"
    }
   },
   "outputs": [],
   "source": [
    "def CropBorders(img):\n",
    "    nrows, ncols = img.shape[0],img.shape[1]\n",
    "    # Get the start and end rows and columns\n",
    "    l_crop = int(ncols * 0.01)\n",
    "    r_crop = int(ncols * (1 - 0.01))\n",
    "    u_crop = int(nrows * 0.04)\n",
    "    d_crop = int(nrows * (1 - 0.04))\n",
    "    cropped_img = img[u_crop:d_crop, l_crop:r_crop]    \n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:13.075126Z",
     "iopub.status.busy": "2022-12-02T04:17:13.074542Z",
     "iopub.status.idle": "2022-12-02T04:17:13.081642Z",
     "shell.execute_reply": "2022-12-02T04:17:13.080772Z",
     "shell.execute_reply.started": "2022-12-02T04:17:13.075087Z"
    }
   },
   "outputs": [],
   "source": [
    "def OwnGlobalBinarise(img, thresh, maxval):       \n",
    "    binarised_img = np.zeros(img.shape, np.uint8)\n",
    "    binarised_img[img >= thresh] = maxval    \n",
    "    return binarised_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:13.280304Z",
     "iopub.status.busy": "2022-12-02T04:17:13.279997Z",
     "iopub.status.idle": "2022-12-02T04:17:13.286320Z",
     "shell.execute_reply": "2022-12-02T04:17:13.285280Z",
     "shell.execute_reply.started": "2022-12-02T04:17:13.280277Z"
    }
   },
   "outputs": [],
   "source": [
    "def OpenMask(mask, ksize=(23, 23), operation=\"open\"):\n",
    "    kernel = cv2.getStructuringElement(shape=cv2.MORPH_RECT, ksize=ksize)    \n",
    "    if operation == \"open\":\n",
    "        edited_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    elif operation == \"close\":\n",
    "        edited_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)    \n",
    "    # Then dilate\n",
    "    edited_mask = cv2.morphologyEx(edited_mask, cv2.MORPH_DILATE, kernel)\n",
    "    return edited_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:13.506793Z",
     "iopub.status.busy": "2022-12-02T04:17:13.505807Z",
     "iopub.status.idle": "2022-12-02T04:17:13.513114Z",
     "shell.execute_reply": "2022-12-02T04:17:13.511960Z",
     "shell.execute_reply.started": "2022-12-02T04:17:13.506750Z"
    }
   },
   "outputs": [],
   "source": [
    "def SortContoursByArea(contours, reverse=True):   \n",
    "    '''\n",
    "    ----------\n",
    "    contours : {list}\n",
    "        The list of contours to sort.        \n",
    "    Returns\n",
    "    -------\n",
    "    sorted_contours : {list}\n",
    "        The list of contours sorted by contour area in descending\n",
    "        order.\n",
    "    bounding_boxes : {list}\n",
    "        The list of bounding boxes ordered corresponding to the\n",
    "        contours in `sorted_contours`.\n",
    "    '''   \n",
    "    # Sort contours based on contour area.\n",
    "    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)    \n",
    "    # Construct the list of corresponding bounding boxes.\n",
    "    bounding_boxes = [cv2.boundingRect(c) for c in sorted_contours]\n",
    "    return sorted_contours, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:13.947784Z",
     "iopub.status.busy": "2022-12-02T04:17:13.946911Z",
     "iopub.status.idle": "2022-12-02T04:17:13.955455Z",
     "shell.execute_reply": "2022-12-02T04:17:13.954341Z",
     "shell.execute_reply.started": "2022-12-02T04:17:13.947735Z"
    }
   },
   "outputs": [],
   "source": [
    "def DrawContourID(img, bounding_box, contour_id):    \n",
    "    '''\n",
    "    ----------\n",
    "    img: {numpy.ndarray}\n",
    "        The image to draw the contour on.\n",
    "    bounding_box : {tuple of int}\n",
    "        The bounding_rect of the given contour.\n",
    "    contour_id : {int or float}\n",
    "        The corresponding ID of the given `contour`.        \n",
    "    Returns\n",
    "    -------\n",
    "    img : {numpy.ndarray}\n",
    "        The image after the `contour` and its ID is drawn on.\n",
    "    ''' \n",
    "    # Center of bounding_rect.\n",
    "    x, y, w, h = bounding_box\n",
    "    center = ( ((x + w) // 2), ((y + h) // 2) )\n",
    "    # Draw the countour number on the image\n",
    "    cv2.putText(img=img,\n",
    "                text=f\"{contour_id}\",\n",
    "                org=center, # Bottom-left corner of the text string in the image.\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=10, \n",
    "                color=(255, 255, 255),\n",
    "                thickness=40)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:14.508741Z",
     "iopub.status.busy": "2022-12-02T04:17:14.508335Z",
     "iopub.status.idle": "2022-12-02T04:17:14.523191Z",
     "shell.execute_reply": "2022-12-02T04:17:14.522277Z",
     "shell.execute_reply.started": "2022-12-02T04:17:14.508705Z"
    }
   },
   "outputs": [],
   "source": [
    "def XLargestBlobs(mask, top_X=None):\n",
    "    \n",
    "    '''\n",
    "    ----------\n",
    "    mask : {numpy.ndarray, dtype=np.uint8}\n",
    "        The mask to get the top X largest blobs.\n",
    "    top_X : {int}\n",
    "        The top X contours to keep based on contour area\n",
    "        ranked in decesnding order.\n",
    "    Returns\n",
    "    -------\n",
    "    n_contours : {int}\n",
    "        The number of contours found in the given `mask`.\n",
    "    X_largest_blobs : {numpy.ndarray}\n",
    "        The corresponding mask of the image containing only\n",
    "        the top X largest contours in white.\n",
    "    '''        \n",
    "    # Find all contours from binarised image.\n",
    "    # Note: parts of the image that you want to get should be white.\n",
    "    contours, hierarchy = cv2.findContours(image=mask,\n",
    "                                           mode=cv2.RETR_EXTERNAL,\n",
    "                                           method=cv2.CHAIN_APPROX_NONE)   \n",
    "    n_contours = len(contours)    \n",
    "    # Only get largest blob if there is at least 1 contour.\n",
    "    if n_contours > 0:        \n",
    "        # Make sure that the number of contours to keep is at most equal \n",
    "        # to the number of contours present in the mask.\n",
    "        if n_contours < top_X or top_X == None:\n",
    "            top_X = n_contours        \n",
    "        # Sort contours based on contour area.\n",
    "        sorted_contours, bounding_boxes = SortContoursByArea(contours=contours,\n",
    "                                                             reverse=True)        \n",
    "        # Get the top X largest contours.\n",
    "        X_largest_contours = sorted_contours[0:top_X]        \n",
    "        # Create black canvas to draw contours on.\n",
    "        to_draw_on = np.zeros(mask.shape, np.uint8)        \n",
    "        # Draw contours in X_largest_contours.\n",
    "        X_largest_blobs = cv2.drawContours(image=to_draw_on, # Draw the contours on `to_draw_on`.\n",
    "                                           contours=X_largest_contours, # List of contours to draw.\n",
    "                                           contourIdx=-1, # Draw all contours in `contours`.\n",
    "                                           color=1, # Draw the contours in white.\n",
    "                                           thickness=-1) # Thickness of the contour lines.        \n",
    "    return n_contours, X_largest_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:15.417393Z",
     "iopub.status.busy": "2022-12-02T04:17:15.416713Z",
     "iopub.status.idle": "2022-12-02T04:17:15.422898Z",
     "shell.execute_reply": "2022-12-02T04:17:15.421612Z",
     "shell.execute_reply.started": "2022-12-02T04:17:15.417355Z"
    }
   },
   "outputs": [],
   "source": [
    "def ApplyMask(img, mask):   \n",
    "    '''\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to mask.\n",
    "    mask : {numpy.ndarray, dtype=np.uint8}\n",
    "        The mask to apply.        \n",
    "    Returns\n",
    "    -------\n",
    "    masked_img: {numpy.ndarray}\n",
    "        The masked image.\n",
    "    '''   \n",
    "    masked_img = img.copy()\n",
    "    masked_img[mask == 0] = 0    \n",
    "    return masked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:16.151420Z",
     "iopub.status.busy": "2022-12-02T04:17:16.151066Z",
     "iopub.status.idle": "2022-12-02T04:17:16.158123Z",
     "shell.execute_reply": "2022-12-02T04:17:16.157146Z",
     "shell.execute_reply.started": "2022-12-02T04:17:16.151380Z"
    }
   },
   "outputs": [],
   "source": [
    "def HorizontalFlip(mask):    \n",
    "    '''\n",
    "    ----------\n",
    "    mask : {numpy.ndarray, dtype=np.uint8}\n",
    "        The corresponding mask of the CC image to flip.\n",
    "    Returns\n",
    "    -------\n",
    "    horizontal_flip : {boolean}\n",
    "        True means need to flip horizontally,\n",
    "        False means otherwise.\n",
    "    '''    \n",
    "    # Get number of rows and columns in the image.\n",
    "    nrows, ncols = mask.shape\n",
    "    x_center = ncols // 2\n",
    "    y_center = nrows // 2    \n",
    "    # Sum down each column.\n",
    "    col_sum = mask.sum(axis=0)\n",
    "    # Sum across each row.\n",
    "    row_sum = mask.sum(axis=1)    \n",
    "    left_sum = sum(col_sum[0:x_center])\n",
    "    right_sum = sum(col_sum[x_center:-1])\n",
    "    top_sum = sum(row_sum[0:y_center])\n",
    "    bottom_sum = sum(row_sum[y_center:-1])    \n",
    "    if left_sum < right_sum:\n",
    "        horizontal_flip = True\n",
    "    else:\n",
    "        horizontal_flip = False        \n",
    "    return horizontal_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:16.873512Z",
     "iopub.status.busy": "2022-12-02T04:17:16.872797Z",
     "iopub.status.idle": "2022-12-02T04:17:16.879942Z",
     "shell.execute_reply": "2022-12-02T04:17:16.878891Z",
     "shell.execute_reply.started": "2022-12-02T04:17:16.873474Z"
    }
   },
   "outputs": [],
   "source": [
    "def clahe(img, clip=2.0, tile=(8, 8)):   \n",
    "    '''\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to edit.\n",
    "    clip : {int or floa}\n",
    "        Threshold for contrast limiting.\n",
    "    tile : {tuple (int, int)}\n",
    "        Size of grid for histogram equalization. Input\n",
    "        image will be divided into equally sized\n",
    "        rectangular tiles. `tile` defines the number of\n",
    "        tiles in row and column.    \n",
    "    Returns\n",
    "    -------\n",
    "    clahe_img : {numpy.ndarray}\n",
    "        The edited image.\n",
    "   '''    \n",
    "    # Convert to uint8.\n",
    "    # img = skimage.img_as_ubyte(img)\n",
    "    img = cv2.normalize(\n",
    "        img,\n",
    "        None,\n",
    "        alpha=0,\n",
    "        beta=255,\n",
    "        norm_type=cv2.NORM_MINMAX,\n",
    "        dtype=cv2.CV_32F,\n",
    "    )\n",
    "    img_uint8 = img.astype(\"uint8\")\n",
    "    clahe_create = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    clahe_img = clahe_create.apply(img_uint8)\n",
    "    return clahe_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:17.618539Z",
     "iopub.status.busy": "2022-12-02T04:17:17.617817Z",
     "iopub.status.idle": "2022-12-02T04:17:17.625550Z",
     "shell.execute_reply": "2022-12-02T04:17:17.624351Z",
     "shell.execute_reply.started": "2022-12-02T04:17:17.618502Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad(img):\n",
    "    nrows, ncols, nchannels = img.shape\n",
    "    # If padding is required...\n",
    "    if nrows != ncols:\n",
    "        # Take the longer side as the target shape.\n",
    "        if ncols < nrows:\n",
    "            target_shape = (nrows, nrows,nchannels)\n",
    "        elif nrows < ncols:\n",
    "            target_shape = (ncols, ncols,nchannels)\n",
    "        # pad.\n",
    "        padded_img = np.zeros(shape=target_shape)\n",
    "        padded_img[:nrows, :ncols, :] = img\n",
    "    # If padding is not required...\n",
    "    elif nrows == ncols:\n",
    "        # Return original image.\n",
    "        padded_img = img\n",
    "    return cv2.resize(padded_img,dim)\n",
    "\t\n",
    "#############################################\n",
    "#### Define ImageDataGenerator Functions ####\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:17:19.333656Z",
     "iopub.status.busy": "2022-12-02T04:17:19.332750Z",
     "iopub.status.idle": "2022-12-02T04:17:19.345612Z",
     "shell.execute_reply": "2022-12-02T04:17:19.344535Z",
     "shell.execute_reply.started": "2022-12-02T04:17:19.333617Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_fun(img):\n",
    "    #Combines all the transformations\n",
    "    #img = cv2.imread(filename)\n",
    "    crop_img = CropBorders(img[:,:,0])\n",
    "    norm = np.zeros((crop_img.shape[0],crop_img.shape[1]))\n",
    "    crop_img=cv2.normalize( crop_img,norm,alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    # Changed bin_img to 0.25 from 0.15\n",
    "    bin_img = OwnGlobalBinarise(img=crop_img,thresh=0.20, maxval=1.0)\n",
    "    mask_img = OpenMask(mask=bin_img, ksize=(2, 2), operation=\"open\")\n",
    "    _, blob_img = XLargestBlobs(mask=np.array(mask_img), top_X=1)\n",
    "    processed_img = ApplyMask(img=crop_img, mask=blob_img)\n",
    "    horizontal_flip = HorizontalFlip(mask=blob_img)\n",
    "    if horizontal_flip:\n",
    "        flipped_img = np.fliplr(processed_img)\n",
    "    else:\n",
    "        flipped_img = processed_img\n",
    "    norm = np.zeros((flipped_img.shape[0],flipped_img.shape[1]))\n",
    "    norm_image = cv2.normalize(flipped_img,norm,0,255,cv2.NORM_MINMAX)\n",
    "    clahe_cv = cv2.createCLAHE(clipLimit =2.0, tileGridSize=(8,8))\n",
    "    norm_image=clahe_cv.apply(norm_image.astype(np.uint8))\n",
    "    #norm_image=cv2.normalize(norm_image,norm,alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    norm3=np.zeros([norm_image.shape[0],norm_image.shape[1],3])\n",
    "    norm3[:,:,0]=norm_image\n",
    "    norm3[:,:,1]=norm_image\n",
    "    norm3[:,:,2]=norm_image\n",
    "    norm3=cv2.resize(norm3,(300,600))\n",
    "    return norm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:22:43.578606Z",
     "iopub.status.busy": "2022-12-02T04:22:43.578235Z",
     "iopub.status.idle": "2022-12-02T04:22:43.589052Z",
     "shell.execute_reply": "2022-12-02T04:22:43.587984Z",
     "shell.execute_reply.started": "2022-12-02T04:22:43.578574Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiple_image_data_generator(generator,directory, batch_size, img_height,img_width):\n",
    "    \n",
    "    generator_X = generator.flow_from_directory(directory,\n",
    "                                          target_size = (img_width, img_height),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        #x=\n",
    "        x1,y1 = (generator_X[0])\n",
    "        x2,y2 = (generator_X.next())\n",
    "        x3,y3 = (generator_X.next())\n",
    "        x4,y4 = (generator_X.next())\n",
    "        x1=tf.expand_dims(x1,1)\n",
    "        x2=tf.expand_dims(x2,1)\n",
    "        x3=tf.expand_dims(x3,1)\n",
    "        x4=tf.expand_dims(x4,1)\n",
    "        #y1=tf.expand_dims(y1,1)\n",
    "        #y2=tf.expand_dims(y2,1)\n",
    "        #y3=tf.expand_dims(y3,1)\n",
    "        #y4=tf.expand_dims(y4,1)\n",
    "\n",
    "        x=tf.concat([x1,x2,x3,x4], 1)\n",
    "        y=tf.concat([y1,y2,y3,y4], 1)\n",
    "\n",
    "        yield x,y #Yield all images and their mutual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:22:43.797514Z",
     "iopub.status.busy": "2022-12-02T04:22:43.797185Z",
     "iopub.status.idle": "2022-12-02T04:22:43.804386Z",
     "shell.execute_reply": "2022-12-02T04:22:43.803234Z",
     "shell.execute_reply.started": "2022-12-02T04:22:43.797485Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "     shear_range=0.4,\n",
    "     zoom_range=0.4,\n",
    "     rotation_range=0.4,\n",
    "     width_shift_range=0.4,\n",
    "     height_shift_range=0.4,\n",
    "     horizontal_flip=True,\n",
    "     vertical_flip=True,\n",
    "    \n",
    "     fill_mode=\"nearest\"\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255)\n",
    "\n",
    "train_generator = multiple_image_data_generator(train_datagen,\n",
    "                                                'D:/Datasets/Breast/Miniddsm-53gb/archive/MINI-DDSM-Complete-JPEG-8',\n",
    "                                                4, height, width)\n",
    "val_generator = multiple_image_data_generator(val_datagen,\n",
    "                                                'D:/Datasets/Breast/Miniddsm-53gb/archive/MINI-DDSM-Complete-JPEG-8-valid',\n",
    "                                                4, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:22:36.111881Z",
     "iopub.status.busy": "2022-12-02T04:22:36.111501Z",
     "iopub.status.idle": "2022-12-02T04:22:36.326605Z",
     "shell.execute_reply": "2022-12-02T04:22:36.325541Z",
     "shell.execute_reply.started": "2022-12-02T04:22:36.111830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 620 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "x,y=next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:22:37.187857Z",
     "iopub.status.busy": "2022-12-02T04:22:37.186960Z",
     "iopub.status.idle": "2022-12-02T04:22:37.197148Z",
     "shell.execute_reply": "2022-12-02T04:22:37.196011Z",
     "shell.execute_reply.started": "2022-12-02T04:22:37.187794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 12), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:22:39.637320Z",
     "iopub.status.busy": "2022-12-02T04:22:39.636947Z",
     "iopub.status.idle": "2022-12-02T04:22:39.877705Z",
     "shell.execute_reply": "2022-12-02T04:22:39.876691Z",
     "shell.execute_reply.started": "2022-12-02T04:22:39.637288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 12), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0,2,:,:,:])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 12)                120064119 \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               3584      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               66048     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               33024     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,169,722\n",
      "Trainable params: 119,954,810\n",
      "Non-trainable params: 214,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 'C:/Users/marcb/Downloads/miniddsm2/MINI-DDSM-Complete-JPEG-8-valid',\n",
    "# val_datagen = ImageDataGenerator(rescale=1. / 255) \t\n",
    "\n",
    "# valid_generator =multiple_image_data_generator(val_datagen,\n",
    "#                                                 'D:/Datasets/Breast/Miniddsm-53gb/archive/MINI-DDSM-Complete-JPEG-8-valid',\n",
    "#                                                  4, height, width)\n",
    "#valid_generator =single_image_data_generator(val_datagen,\n",
    "#                                                'D:/Datasets/Breast/Miniddsm-53gb/archive/MINI-DDSM-Complete-JPEG-8-valid',\n",
    "#                                                 4, height, width)\n",
    "# valid_generator = val_datagen.flow_from_directory(\n",
    "#     'D:/Datasets/Breast/Miniddsm-53gb/archive/MINI-DDSM-Complete-JPEG-8-valid',\n",
    "#     target_size=(600,300), \n",
    "#     interpolation=\"lanczos\",\n",
    "#     color_mode='rgb', \n",
    "#     batch_size=4, \n",
    "#     class_mode='categorical', \n",
    "#     shuffle=True)\n",
    "    \n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "#model.add()\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(256,activation='PReLU'))\n",
    "#model.add(Dropout(.5))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(256,activation='PReLU'))\n",
    "#model.add(Dropout(.5))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(128,activation='PReLU'))\n",
    "#model.add(Dropout(.5))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dense(3,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def auc_score(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.numpy_function(auc_score, (y_true, y_pred), tf.double)\n",
    "#in model.compile you can use auc function name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:18:51.696365Z",
     "iopub.status.busy": "2022-12-02T04:18:51.695862Z",
     "iopub.status.idle": "2022-12-02T04:18:51.709818Z",
     "shell.execute_reply": "2022-12-02T04:18:51.708949Z",
     "shell.execute_reply.started": "2022-12-02T04:18:51.696325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=4\n",
    "n_train=8800\n",
    "n_test=879\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T04:22:52.560770Z",
     "iopub.status.busy": "2022-12-02T04:22:52.560407Z",
     "iopub.status.idle": "2022-12-02T04:35:49.521355Z",
     "shell.execute_reply": "2022-12-02T04:35:49.519716Z",
     "shell.execute_reply.started": "2022-12-02T04:22:52.560739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6588 images belonging to 3 classes.\n",
      "Epoch 1/90\n",
      "550/550 [==============================] - 285s 464ms/step - loss: 8.4473 - categorical_accuracy: 0.8786 - auc: 0.9221 - val_loss: 25.5883 - val_categorical_accuracy: 1.0000 - val_auc: 0.5958\n",
      "Epoch 2/90\n",
      "550/550 [==============================] - 233s 423ms/step - loss: 9.2294 - categorical_accuracy: 0.9400 - auc: 0.8300 - val_loss: 19.8906 - val_categorical_accuracy: 1.0000 - val_auc: 0.6062\n",
      "Epoch 3/90\n",
      "550/550 [==============================] - 221s 403ms/step - loss: 9.3882 - categorical_accuracy: 0.9782 - auc: 0.6218 - val_loss: 9.7038 - val_categorical_accuracy: 1.0000 - val_auc: 0.6233\n",
      "Epoch 4/90\n",
      "550/550 [==============================] - 222s 403ms/step - loss: 9.2102 - categorical_accuracy: 0.9868 - auc: 0.6249 - val_loss: 9.9359 - val_categorical_accuracy: 1.0000 - val_auc: 0.6241\n",
      "Epoch 5/90\n",
      "550/550 [==============================] - 225s 410ms/step - loss: 9.1506 - categorical_accuracy: 0.9918 - auc: 0.6350 - val_loss: 15.2560 - val_categorical_accuracy: 1.0000 - val_auc: 0.6028\n",
      "Epoch 6/90\n",
      "550/550 [==============================] - 227s 413ms/step - loss: 9.3218 - categorical_accuracy: 0.9955 - auc: 0.6429 - val_loss: 10.3894 - val_categorical_accuracy: 1.0000 - val_auc: 0.6223\n",
      "Epoch 7/90\n",
      "550/550 [==============================] - 218s 396ms/step - loss: 9.2105 - categorical_accuracy: 0.9973 - auc: 0.6248 - val_loss: 10.0726 - val_categorical_accuracy: 1.0000 - val_auc: 0.6236\n",
      "Epoch 8/90\n",
      "550/550 [==============================] - 206s 374ms/step - loss: 9.2102 - categorical_accuracy: 0.9964 - auc: 0.6251 - val_loss: 11.1597 - val_categorical_accuracy: 1.0000 - val_auc: 0.6225\n",
      "Epoch 9/90\n",
      "550/550 [==============================] - 206s 374ms/step - loss: 9.2083 - categorical_accuracy: 0.9991 - auc: 0.6254 - val_loss: 10.2445 - val_categorical_accuracy: 1.0000 - val_auc: 0.6237\n",
      "Epoch 10/90\n",
      "550/550 [==============================] - 218s 396ms/step - loss: 15.5128 - categorical_accuracy: 0.9986 - auc: 0.6367 - val_loss: 15.0783 - val_categorical_accuracy: 1.0000 - val_auc: 0.6125\n",
      "Epoch 11/90\n",
      "550/550 [==============================] - 215s 391ms/step - loss: 10.2267 - categorical_accuracy: 1.0000 - auc: 0.6647 - val_loss: 13.7848 - val_categorical_accuracy: 1.0000 - val_auc: 0.6254\n",
      "Epoch 12/90\n",
      "550/550 [==============================] - 213s 388ms/step - loss: 13.7618 - categorical_accuracy: 0.9995 - auc: 0.6391 - val_loss: 17.2022 - val_categorical_accuracy: 1.0000 - val_auc: 0.6340\n",
      "Epoch 13/90\n",
      "550/550 [==============================] - 216s 393ms/step - loss: 17.9921 - categorical_accuracy: 0.9991 - auc: 0.6208 - val_loss: 18.8988 - val_categorical_accuracy: 1.0000 - val_auc: 0.6120\n",
      "Epoch 14/90\n",
      "550/550 [==============================] - 220s 401ms/step - loss: 17.9916 - categorical_accuracy: 0.9986 - auc: 0.6208 - val_loss: 18.8695 - val_categorical_accuracy: 1.0000 - val_auc: 0.6134\n",
      "Epoch 15/90\n",
      "550/550 [==============================] - 217s 395ms/step - loss: 17.9916 - categorical_accuracy: 0.9991 - auc: 0.6208 - val_loss: 18.8134 - val_categorical_accuracy: 1.0000 - val_auc: 0.6135\n",
      "Epoch 16/90\n",
      "550/550 [==============================] - 204s 371ms/step - loss: 17.9916 - categorical_accuracy: 1.0000 - auc: 0.6208 - val_loss: 18.6905 - val_categorical_accuracy: 1.0000 - val_auc: 0.6151\n",
      "Epoch 17/90\n",
      "550/550 [==============================] - 210s 382ms/step - loss: 17.9916 - categorical_accuracy: 0.9986 - auc: 0.6208 - val_loss: 17.0897 - val_categorical_accuracy: 1.0000 - val_auc: 0.6371\n",
      "Epoch 18/90\n",
      "550/550 [==============================] - 217s 394ms/step - loss: 14.2367 - categorical_accuracy: 1.0000 - auc: 0.6166 - val_loss: 14.8553 - val_categorical_accuracy: 1.0000 - val_auc: 0.6218\n",
      "Epoch 19/90\n",
      "550/550 [==============================] - 205s 373ms/step - loss: 13.5980 - categorical_accuracy: 1.0000 - auc: 0.6227 - val_loss: 14.0189 - val_categorical_accuracy: 1.0000 - val_auc: 0.6275\n",
      "Epoch 20/90\n",
      "550/550 [==============================] - 205s 372ms/step - loss: 13.5980 - categorical_accuracy: 0.9995 - auc: 0.6227 - val_loss: 14.1679 - val_categorical_accuracy: 1.0000 - val_auc: 0.6283\n",
      "Epoch 21/90\n",
      "550/550 [==============================] - 205s 372ms/step - loss: 13.5980 - categorical_accuracy: 1.0000 - auc: 0.6227 - val_loss: 14.1397 - val_categorical_accuracy: 1.0000 - val_auc: 0.6235\n",
      "Epoch 22/90\n",
      "550/550 [==============================] - 215s 390ms/step - loss: 13.5980 - categorical_accuracy: 1.0000 - auc: 0.6227 - val_loss: 13.7953 - val_categorical_accuracy: 1.0000 - val_auc: 0.6308\n",
      "Epoch 23/90\n",
      "550/550 [==============================] - 220s 400ms/step - loss: 13.5980 - categorical_accuracy: 1.0000 - auc: 0.6227 - val_loss: 14.2878 - val_categorical_accuracy: 1.0000 - val_auc: 0.6269\n",
      "Epoch 24/90\n",
      "550/550 [==============================] - 206s 374ms/step - loss: 13.5980 - categorical_accuracy: 0.9995 - auc: 0.6227 - val_loss: 14.0370 - val_categorical_accuracy: 1.0000 - val_auc: 0.6279\n",
      "Epoch 25/90\n",
      "550/550 [==============================] - 205s 374ms/step - loss: 13.5980 - categorical_accuracy: 1.0000 - auc: 0.6227 - val_loss: 14.7734 - val_categorical_accuracy: 1.0000 - val_auc: 0.6184\n",
      "Epoch 26/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 13.5980 - categorical_accuracy: 1.0000 - auc: 0.6227 - val_loss: 15.0470 - val_categorical_accuracy: 1.0000 - val_auc: 0.6186\n",
      "Epoch 27/90\n",
      "550/550 [==============================] - 210s 383ms/step - loss: 13.5980 - categorical_accuracy: 0.9991 - auc: 0.6227 - val_loss: 14.9578 - val_categorical_accuracy: 1.0000 - val_auc: 0.6160\n",
      "Epoch 28/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 12.5685 - categorical_accuracy: 1.0000 - auc: 0.5936 - val_loss: 9.8442 - val_categorical_accuracy: 1.0000 - val_auc: 0.6277\n",
      "Epoch 29/90\n",
      "550/550 [==============================] - 210s 383ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.5466 - val_categorical_accuracy: 1.0000 - val_auc: 0.6197\n",
      "Epoch 30/90\n",
      "550/550 [==============================] - 210s 382ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.0474 - val_categorical_accuracy: 1.0000 - val_auc: 0.6237\n",
      "Epoch 31/90\n",
      "550/550 [==============================] - 215s 390ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.3744 - val_categorical_accuracy: 1.0000 - val_auc: 0.6210\n",
      "Epoch 32/90\n",
      "550/550 [==============================] - 213s 387ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.2115 - val_categorical_accuracy: 1.0000 - val_auc: 0.6191\n",
      "Epoch 33/90\n",
      "550/550 [==============================] - 210s 382ms/step - loss: 9.2104 - categorical_accuracy: 0.9995 - auc: 0.6250 - val_loss: 9.8143 - val_categorical_accuracy: 1.0000 - val_auc: 0.6257\n",
      "Epoch 34/90\n",
      "550/550 [==============================] - 211s 383ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.1015 - val_categorical_accuracy: 1.0000 - val_auc: 0.6217\n",
      "Epoch 35/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 0.9995 - auc: 0.6250 - val_loss: 9.8954 - val_categorical_accuracy: 1.0000 - val_auc: 0.6237\n",
      "Epoch 36/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 0.9995 - auc: 0.6250 - val_loss: 9.6061 - val_categorical_accuracy: 1.0000 - val_auc: 0.6253\n",
      "Epoch 37/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.9546 - val_categorical_accuracy: 1.0000 - val_auc: 0.6223\n",
      "Epoch 38/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.8604 - val_categorical_accuracy: 1.0000 - val_auc: 0.6251\n",
      "Epoch 39/90\n",
      "550/550 [==============================] - 212s 385ms/step - loss: 9.2104 - categorical_accuracy: 0.9995 - auc: 0.6250 - val_loss: 9.5833 - val_categorical_accuracy: 1.0000 - val_auc: 0.6266\n",
      "Epoch 40/90\n",
      "550/550 [==============================] - 212s 386ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.0049 - val_categorical_accuracy: 1.0000 - val_auc: 0.6216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/90\n",
      "550/550 [==============================] - 210s 382ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.7428 - val_categorical_accuracy: 1.0000 - val_auc: 0.6237\n",
      "Epoch 42/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.8832 - val_categorical_accuracy: 1.0000 - val_auc: 0.6225\n",
      "Epoch 43/90\n",
      "550/550 [==============================] - 211s 383ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 10.1124 - val_categorical_accuracy: 1.0000 - val_auc: 0.6190\n",
      "Epoch 44/90\n",
      "550/550 [==============================] - 210s 382ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3315 - val_categorical_accuracy: 1.0000 - val_auc: 0.6271\n",
      "Epoch 45/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.7018 - val_categorical_accuracy: 1.0000 - val_auc: 0.6240\n",
      "Epoch 46/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.7216 - val_categorical_accuracy: 1.0000 - val_auc: 0.6227\n",
      "Epoch 47/90\n",
      "550/550 [==============================] - 211s 385ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3896 - val_categorical_accuracy: 1.0000 - val_auc: 0.6251\n",
      "Epoch 48/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2159 - categorical_accuracy: 1.0000 - auc: 0.6249 - val_loss: 9.5643 - val_categorical_accuracy: 1.0000 - val_auc: 0.6254\n",
      "Epoch 49/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.6188 - val_categorical_accuracy: 1.0000 - val_auc: 0.6237\n",
      "Epoch 50/90\n",
      "550/550 [==============================] - 213s 387ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3323 - val_categorical_accuracy: 1.0000 - val_auc: 0.6259\n",
      "Epoch 51/90\n",
      "550/550 [==============================] - 215s 391ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.5100 - val_categorical_accuracy: 1.0000 - val_auc: 0.6234\n",
      "Epoch 52/90\n",
      "550/550 [==============================] - 212s 385ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.5546 - val_categorical_accuracy: 1.0000 - val_auc: 0.6237\n",
      "Epoch 53/90\n",
      "550/550 [==============================] - 211s 383ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3190 - val_categorical_accuracy: 1.0000 - val_auc: 0.6276\n",
      "Epoch 54/90\n",
      "550/550 [==============================] - 212s 385ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.6416 - val_categorical_accuracy: 1.0000 - val_auc: 0.6213\n",
      "Epoch 55/90\n",
      "550/550 [==============================] - 212s 385ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3307 - val_categorical_accuracy: 1.0000 - val_auc: 0.6254\n",
      "Epoch 56/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 0.9995 - auc: 0.6250 - val_loss: 9.4643 - val_categorical_accuracy: 1.0000 - val_auc: 0.6257\n",
      "Epoch 57/90\n",
      "550/550 [==============================] - 211s 384ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.4359 - val_categorical_accuracy: 1.0000 - val_auc: 0.6236\n",
      "Epoch 58/90\n",
      "550/550 [==============================] - 215s 392ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3353 - val_categorical_accuracy: 1.0000 - val_auc: 0.6253\n",
      "Epoch 59/90\n",
      "550/550 [==============================] - 219s 399ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.4844 - val_categorical_accuracy: 1.0000 - val_auc: 0.6263\n",
      "Epoch 60/90\n",
      "550/550 [==============================] - 227s 413ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.4454 - val_categorical_accuracy: 1.0000 - val_auc: 0.6247\n",
      "Epoch 61/90\n",
      "550/550 [==============================] - 224s 408ms/step - loss: 9.2104 - categorical_accuracy: 1.0000 - auc: 0.6250 - val_loss: 9.3310 - val_categorical_accuracy: 1.0000 - val_auc: 0.6261\n",
      "Epoch 62/90\n",
      "550/550 [==============================] - 215s 392ms/step - loss: 9.2079 - categorical_accuracy: 1.0000 - auc: 0.6255 - val_loss: 12.3206 - val_categorical_accuracy: 1.0000 - val_auc: 0.6232\n",
      "Epoch 63/90\n",
      "550/550 [==============================] - 209s 380ms/step - loss: 17.5567 - categorical_accuracy: 1.0000 - auc: 0.6237 - val_loss: 16.9246 - val_categorical_accuracy: 1.0000 - val_auc: 0.6300\n",
      "Epoch 64/90\n",
      "550/550 [==============================] - 215s 392ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.2952 - val_categorical_accuracy: 1.0000 - val_auc: 0.6223\n",
      "Epoch 65/90\n",
      "550/550 [==============================] - 226s 412ms/step - loss: 17.5322 - categorical_accuracy: 0.9995 - auc: 0.6269 - val_loss: 17.6151 - val_categorical_accuracy: 1.0000 - val_auc: 0.6190\n",
      "Epoch 66/90\n",
      "550/550 [==============================] - 241s 439ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6817 - val_categorical_accuracy: 1.0000 - val_auc: 0.6187\n",
      "Epoch 67/90\n",
      "550/550 [==============================] - 221s 401ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.5707 - val_categorical_accuracy: 1.0000 - val_auc: 0.6179\n",
      "Epoch 68/90\n",
      "550/550 [==============================] - 208s 378ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6100 - val_categorical_accuracy: 1.0000 - val_auc: 0.6192\n",
      "Epoch 69/90\n",
      "550/550 [==============================] - 208s 378ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6206 - val_categorical_accuracy: 1.0000 - val_auc: 0.6190\n",
      "Epoch 70/90\n",
      "550/550 [==============================] - 209s 380ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.5525 - val_categorical_accuracy: 1.0000 - val_auc: 0.6195\n",
      "Epoch 71/90\n",
      "550/550 [==============================] - 209s 380ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.4978 - val_categorical_accuracy: 1.0000 - val_auc: 0.6200\n",
      "Epoch 72/90\n",
      "550/550 [==============================] - 208s 378ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.1090 - val_categorical_accuracy: 1.0000 - val_auc: 0.6261\n",
      "Epoch 73/90\n",
      "550/550 [==============================] - 212s 385ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.6393 - val_categorical_accuracy: 1.0000 - val_auc: 0.6300\n",
      "Epoch 74/90\n",
      "550/550 [==============================] - 209s 380ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.9152 - val_categorical_accuracy: 1.0000 - val_auc: 0.6289\n",
      "Epoch 75/90\n",
      "550/550 [==============================] - 209s 379ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.8512 - val_categorical_accuracy: 1.0000 - val_auc: 0.6298\n",
      "Epoch 76/90\n",
      "550/550 [==============================] - 208s 378ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.3244 - val_categorical_accuracy: 1.0000 - val_auc: 0.6209\n",
      "Epoch 77/90\n",
      "550/550 [==============================] - 224s 407ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6760 - val_categorical_accuracy: 1.0000 - val_auc: 0.6198\n",
      "Epoch 78/90\n",
      "550/550 [==============================] - 219s 398ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6253 - val_categorical_accuracy: 1.0000 - val_auc: 0.6184\n",
      "Epoch 79/90\n",
      "550/550 [==============================] - 220s 401ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.5685 - val_categorical_accuracy: 1.0000 - val_auc: 0.6188\n",
      "Epoch 80/90\n",
      "550/550 [==============================] - 216s 392ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6209 - val_categorical_accuracy: 1.0000 - val_auc: 0.6189\n",
      "Epoch 81/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 215s 391ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.5633 - val_categorical_accuracy: 1.0000 - val_auc: 0.6193\n",
      "Epoch 82/90\n",
      "550/550 [==============================] - 215s 391ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6785 - val_categorical_accuracy: 1.0000 - val_auc: 0.6195\n",
      "Epoch 83/90\n",
      "550/550 [==============================] - 216s 393ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.5576 - val_categorical_accuracy: 1.0000 - val_auc: 0.6201\n",
      "Epoch 84/90\n",
      "550/550 [==============================] - 218s 396ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.8316 - val_categorical_accuracy: 1.0000 - val_auc: 0.6292\n",
      "Epoch 85/90\n",
      "550/550 [==============================] - 241s 439ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.9761 - val_categorical_accuracy: 1.0000 - val_auc: 0.6286\n",
      "Epoch 86/90\n",
      "550/550 [==============================] - 3101s 6s/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.8515 - val_categorical_accuracy: 1.0000 - val_auc: 0.6301\n",
      "Epoch 87/90\n",
      "550/550 [==============================] - 262s 476ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 16.6358 - val_categorical_accuracy: 1.0000 - val_auc: 0.6311\n",
      "Epoch 88/90\n",
      "550/550 [==============================] - 232s 421ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.7341 - val_categorical_accuracy: 1.0000 - val_auc: 0.6196\n",
      "Epoch 89/90\n",
      "550/550 [==============================] - 245s 446ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.6178 - val_categorical_accuracy: 1.0000 - val_auc: 0.6198\n",
      "Epoch 90/90\n",
      "550/550 [==============================] - 253s 459ms/step - loss: 17.5322 - categorical_accuracy: 1.0000 - auc: 0.6269 - val_loss: 17.5768 - val_categorical_accuracy: 1.0000 - val_auc: 0.6191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b76008d9d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import metrics\n",
    "\n",
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),loss='categorical_crossentropy', \n",
    "              metrics=[\n",
    "                          'categorical_accuracy', 'AUC'\n",
    "                      ])\n",
    "base_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=round(n_train /(4*batch_size)),\n",
    "        epochs=90,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=round(n_test /(4*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'D:/Datasets/Breast/Miniddsm-53gb/archive/MINI-DDSM-Complete-JPEG-8-test',\n",
    "    target_size=(600,300), \n",
    "    interpolation=\"lanczos\",\n",
    "    color_mode='rgb', \n",
    "    batch_size=16, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True)\n",
    "\n",
    "#y_pred=model.predict_on_batch(test_datagen)\n",
    "test_evaluation = model.evaluate(test_generator,verbose = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(model.metrics_names)):\n",
    "    print(\"%s%s: %.2f%%\" % (\" \",model.metrics_names[i], test_evaluation[i]*100))\n",
    "\n",
    "\n",
    "#print(\"Final Model Accuracy = \", (test_acc))\n",
    "#print('Accuracy:', test_evaluation.accuracy_score())\n",
    "#print('Precision:', test_evaluation.precision_score())\n",
    "#print('Recall:', np.round(metrics.recall_score(y_test[:,0],y_pred, average='weighted'),5))\n",
    "#print('F1 Score:', np.round(metrics.f1_score(y_test[:,0], y_pred, average='weighted'),5))\n",
    "#print('ROC AUC Score:', np.round(metrics.roc_auc_score(y_test[:,0], y_pred,multi_class='ovo', average='weighted'),5))\n",
    "#print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test[:,0], y_pred),5))\n",
    "#print('\\t\\tClassification Report:\\n', metrics.classification_report(y_test[:,0], y_pred,target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
